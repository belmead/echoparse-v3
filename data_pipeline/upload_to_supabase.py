"""
upload_to_supabase.py

This script uploads the embedded review data from CSV to Supabase, completely
replacing the existing app_reviews table data while preserving dashboard_metrics.

Prerequisites:
- Embedded CSV file generated by embed.py
- Supabase database credentials in .env file
- psycopg2 and pandas installed

Usage: python upload_to_supabase.py
"""

import pandas as pd
import psycopg2
import psycopg2.extras
import os
from dotenv import load_dotenv
import logging
from typing import List, Dict, Any
import subprocess
import sys

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_db_connection():
    """Create connection to Supabase PostgreSQL database"""
    try:
        # Try DATABASE_URL first (if using connection string)
        database_url = os.getenv('DATABASE_URL')
        if database_url:
            return psycopg2.connect(database_url)
        
        # Otherwise use individual connection parameters
        return psycopg2.connect(
            host=os.getenv('SUPABASE_HOST'),
            database=os.getenv('SUPABASE_DATABASE', 'postgres'),
            user=os.getenv('SUPABASE_USER', 'postgres'),
            password=os.getenv('SUPABASE_PASSWORD'),
            port=os.getenv('SUPABASE_PORT', '5432'),
            sslmode='require'
        )
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        raise

def load_embedded_data(csv_file: str) -> pd.DataFrame:
    """Load embedded review data from CSV file"""
    try:
        df = pd.read_csv(csv_file)
        logger.info(f"Loaded {len(df)} rows from {csv_file}")
        
        # Validate required columns exist
        required_columns = ['author_name', 'rating', 'review_text', 'review_date', 
                          'app_version', 'platform', 'prepared_text', 'embedding']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Convert review_date to proper datetime format if needed
        df['review_date'] = pd.to_datetime(df['review_date'])
        
        # Handle null values
        df = df.fillna({
            'author_name': 'Unknown',
            'app_version': 'Unknown',
            'prepared_text': '',
            'sentiment_score': 0.0
        })
        
        return df
        
    except Exception as e:
        logger.error(f"Error loading CSV file: {e}")
        raise

def clear_app_reviews_table(connection):
    """Clear all existing data from app_reviews table"""
    try:
        cursor = connection.cursor()
        cursor.execute("DELETE FROM app_reviews;")
        connection.commit()
        cursor.close()
        logger.info("Successfully cleared app_reviews table")
        
    except Exception as e:
        logger.error(f"Error clearing app_reviews table: {e}")
        raise

def upload_reviews_batch(connection, df: pd.DataFrame, batch_size: int = 1000):
    """Upload reviews to Suibase in batches"""
    try:
        cursor = connection.cursor()
        
        # Prepare the INSERT statement
        insert_query = """
            INSERT INTO app_reviews (
                author_name, rating, review_text, review_date, 
                app_version, platform, prepared_text, embedding, sentiment_score
            ) VALUES %s
        """
        
        total_rows = len(df)
        rows_processed = 0
        
        for i in range(0, total_rows, batch_size):
            batch = df.iloc[i:i+batch_size]
            
            # Convert batch to list of tuples for psycopg2
            batch_data = []
            for _, row in batch.iterrows():
                # Convert embedding string to list if it's a string
                embedding = row['embedding']
                if isinstance(embedding, str):
                    try:
                        # Parse the embedding string (assuming it's a string representation of a list)
                        import ast
                        embedding = ast.literal_eval(embedding)
                    except:
                        # If parsing fails, set to empty list
                        embedding = []
                
                batch_data.append((
                    row['author_name'],
                    int(row['rating']),
                    row['review_text'],
                    row['review_date'],
                    row['app_version'],
                    row['platform'],
                    row['prepared_text'],
                    embedding,
                    row.get('sentiment_score', 0.0)  # Default to 0.0 if not present
                ))
            
            # Execute batch insert
            psycopg2.extras.execute_values(
                cursor, insert_query, batch_data, template=None, page_size=100
            )
            
            rows_processed += len(batch)
            logger.info(f"Uploaded batch {i//batch_size + 1}: {rows_processed}/{total_rows} rows")
        
        connection.commit()
        cursor.close()
        logger.info(f"Successfully uploaded {rows_processed} reviews to database")
        
    except Exception as e:
        logger.error(f"Error uploading reviews: {e}")
        connection.rollback()
        raise

def verify_upload(connection) -> Dict[str, Any]:
    """Verify the upload was successful by checking counts and sample data"""
    try:
        cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get row count
        cursor.execute("SELECT COUNT(*) as total_rows FROM app_reviews;")
        total_rows = cursor.fetchone()['total_rows']
        
        # Get platform breakdown
        cursor.execute("""
            SELECT platform, COUNT(*) as count 
            FROM app_reviews 
            GROUP BY platform;
        """)
        platform_counts = cursor.fetchall()
        
        # Get date range
        cursor.execute("""
            SELECT MIN(review_date) as earliest_date, MAX(review_date) as latest_date
            FROM app_reviews;
        """)
        date_range = cursor.fetchone()
        
        # Get sample record
        cursor.execute("SELECT * FROM app_reviews LIMIT 1;")
        sample_record = cursor.fetchone()
        
        cursor.close()
        
        return {
            "total_rows": total_rows,
            "platform_counts": dict(platform_counts),
            "date_range": {
                "earliest": date_range['earliest_date'],
                "latest": date_range['latest_date']
            },
            "sample_record": dict(sample_record) if sample_record else None
        }
        
    except Exception as e:
        logger.error(f"Error verifying upload: {e}")
        raise

def calculate_dashboard_metrics():
    """Run the calculate_metrics.py script to update dashboard metrics"""
    try:
        # Check if calculate_metrics.py exists
        if not os.path.exists("calculate_metrics.py"):
            logger.warning("calculate_metrics.py not found. Skipping metrics calculation.")
            return
        
        # Run the metrics calculation script
        result = subprocess.run([sys.executable, "calculate_metrics.py"], 
                              capture_output=True, text=True, check=True)
        
        # Log the output from the metrics script
        if result.stdout:
            for line in result.stdout.strip().split('\n'):
                logger.info(f"METRICS: {line}")
                
    except subprocess.CalledProcessError as e:
        logger.error(f"Error running calculate_metrics.py: {e}")
        if e.stderr:
            logger.error(f"METRICS ERROR: {e.stderr}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error calculating metrics: {e}")
        raise

def main():
    """Main upload process"""
    try:
        logger.info("Starting upload process...")
        
        # Load embedded data from CSV
        csv_file = "embedded_reviews.csv"  # Default filename from embed.py
        if not os.path.exists(csv_file):
            # Try the edited version if it exists
            csv_file = "embedded_reviews_edited.csv"
            if not os.path.exists(csv_file):
                raise FileNotFoundError(f"Embedded CSV file not found. Expected 'embedded_reviews.csv' or 'embedded_reviews_edited.csv'")
        
        df = load_embedded_data(csv_file)
        
        # Connect to database
        logger.info("Connecting to Supabase database...")
        conn = get_db_connection()
        
        # Clear existing data
        logger.info("Clearing existing app_reviews data...")
        clear_app_reviews_table(conn)
        
        # Upload new data
        logger.info("Uploading new embedded review data...")
        upload_reviews_batch(conn, df)
        
        # Verify the upload
        logger.info("Verifying upload...")
        verification = verify_upload(conn)
        
        # Close connection
        conn.close()
        
        # Print summary
        logger.info("\n=== UPLOAD SUMMARY ===")
        logger.info(f"Total rows uploaded: {verification['total_rows']}")
        logger.info(f"Platform breakdown: {verification['platform_counts']}")
        logger.info(f"Date range: {verification['date_range']['earliest']} to {verification['date_range']['latest']}")
        logger.info("Upload completed successfully!")
        
        # Automatically calculate dashboard metrics
        logger.info("\nðŸ”„ Calculating dashboard metrics...")
        calculate_dashboard_metrics()
        logger.info("âœ… Dashboard metrics updated successfully!")
        
    except Exception as e:

        logger.error(f"Upload failed: {e}")
        raise

if __name__ == "__main__":
    main()